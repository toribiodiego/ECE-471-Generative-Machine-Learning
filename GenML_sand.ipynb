{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10863411,"sourceType":"datasetVersion","datasetId":6748692},{"sourceId":10864453,"sourceType":"datasetVersion","datasetId":6749465},{"sourceId":10864550,"sourceType":"datasetVersion","datasetId":6748958}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pillow-heif ipycanvas","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-02-26T21:55:36.383693Z","iopub.execute_input":"2025-02-26T21:55:36.384031Z","iopub.status.idle":"2025-02-26T21:55:39.967002Z","shell.execute_reply.started":"2025-02-26T21:55:36.384002Z","shell.execute_reply":"2025-02-26T21:55:39.966136Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"since the sample dataset is HEI, in order to allow our team members to experiment we can (1) add some codeblock to convert all of the images to png (2) adapt this code so it runs on Google c","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/sand-samples/sand","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T21:45:34.283717Z","iopub.execute_input":"2025-02-26T21:45:34.284061Z","iopub.status.idle":"2025-02-26T21:45:34.421019Z","shell.execute_reply.started":"2025-02-26T21:45:34.284034Z","shell.execute_reply":"2025-02-26T21:45:34.419974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### File format coversion","metadata":{}},{"cell_type":"code","source":"# style_transfer_lib.py\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom PIL import Image\n\n# Set the main device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_image(image_path, target_size=512):\n    image = Image.open(image_path).convert('RGB')\n    transform_pipeline = transforms.Compose([\n        transforms.Resize(target_size),      # Resize the smaller edge to target_size\n        transforms.CenterCrop(target_size),    # Center crop to target_size x target_size\n        transforms.ToTensor(),                 # Convert image to tensor in [0,1] range\n    ])\n    image_tensor = transform_pipeline(image).unsqueeze(0)  # Shape: [1, C, H, W]\n    return image_tensor.to(device)\n\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        # Reshape mean and std for proper broadcasting\n        self.mean = torch.tensor(mean).view(-1, 1, 1).to(device)\n        self.std  = torch.tensor(std).view(-1, 1, 1).to(device)\n    def forward(self, img):\n        return (img - self.mean) / self.std\n\nclass ContentLoss(nn.Module):\n    def __init__(self, target):\n        super(ContentLoss, self).__init__()\n        self.target = target.detach()\n        self.loss = None\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input\n\ndef gram_matrix(input_tensor):\n    b, ch, h, w = input_tensor.size()\n    features = input_tensor.view(b * ch, h * w)\n    G = torch.mm(features, features.t())\n    return G.div(b * ch * h * w)\n\nclass StyleLoss(nn.Module):\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n        self.loss = None\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input\n\n# Default layers at which to compute losses\ncontent_layers_default = ['conv_4']\nstyle_layers_default   = ['conv_1','conv_2','conv_3','conv_4','conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    \n    normalization = Normalization(normalization_mean, normalization_std)\n    content_losses = []\n    style_losses = []\n\n    model = nn.Sequential(normalization)\n    i = 0  # increment for each conv layer\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = f'conv_{i}'\n        elif isinstance(layer, nn.ReLU):\n            name = f'relu_{i}'\n            layer = nn.ReLU(inplace=False)  # Replace in-place ReLU\n        elif isinstance(layer, nn.MaxPool2d):\n            name = f'pool_{i}'\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = f'bn_{i}'\n        else:\n            raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(f\"content_loss_{i}\", content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(f\"style_loss_{i}\", style_loss)\n            style_losses.append(style_loss)\n\n    # Trim layers after the last loss module\n    for j in range(len(model) - 1, -1, -1):\n        if isinstance(model[j], ContentLoss) or isinstance(model[j], StyleLoss):\n            break\n    model = model[:(j + 1)]\n    \n    return model, style_losses, content_losses\n\ndef get_input_optimizer(input_img):\n    # Use LBFGS as in the PyTorch tutorial\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer\n\ndef run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, \n                       num_steps=300, style_weight=1_000_000, content_weight=1):\n    \n    model, style_losses, content_losses = get_style_model_and_losses(\n        cnn, normalization_mean, normalization_std, style_img, content_img\n    )\n    \n    optimizer = get_input_optimizer(input_img)\n    run = [0]\n\n    while run[0] <= num_steps:\n        def closure():\n            with torch.no_grad():\n                input_img.clamp_(0, 1)\n            optimizer.zero_grad()\n            model(input_img)\n            \n            style_score = 0.0\n            content_score = 0.0\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n            \n            loss = style_weight * style_score + content_weight * content_score\n            loss.backward()\n            \n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(f\"Iteration {run[0]}: Style Loss {style_score.item():.4f}, Content Loss {content_score.item():.4f}\")\n            return loss\n        optimizer.step(closure)\n    \n    with torch.no_grad():\n        input_img.clamp_(0, 1)\n    \n    return input_img\n\ndef process_style_transfer(config):\n    \"\"\"\n    Process all PNG content images from config[\"input_dir\"] using the first PNG\n    style image found in config[\"style_dir\"]. The stylized images will be saved\n    in config[\"output_dir\"].\n    \"\"\"\n    # Load style image (only use the first PNG found)\n    style_files = [f for f in os.listdir(config[\"style_dir\"]) if f.lower().endswith(\".png\")]\n    if not style_files:\n        raise ValueError(\"No PNG style images found in the style directory.\")\n    style_image_path = os.path.join(config[\"style_dir\"], style_files[0])\n    style_img = load_image(style_image_path, target_size=config.get(\"target_size\", 512))\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(config[\"output_dir\"], exist_ok=True)\n    \n    # Load the CNN model (default VGG19)\n    cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n    cnn_mean = [0.485, 0.456, 0.406]\n    cnn_std  = [0.229, 0.224, 0.225]\n    \n    # Process each PNG image in the input directory\n    input_files = [f for f in os.listdir(config[\"input_dir\"]) if f.lower().endswith(\".png\")]\n    if not input_files:\n        raise ValueError(\"No PNG content images found in the input directory.\")\n    \n    for img_file in input_files:\n        print(f\"Processing {img_file}...\")\n        content_image_path = os.path.join(config[\"input_dir\"], img_file)\n        content_img = load_image(content_image_path, target_size=config.get(\"target_size\", 512))\n        input_img = content_img.clone()\n        \n        output_tensor = run_style_transfer(\n            cnn, cnn_mean, cnn_std,\n            content_img, style_img, input_img,\n            num_steps=config.get(\"num_steps\", 300),\n            style_weight=config.get(\"style_weight\", 1_000_000),\n            content_weight=config.get(\"content_weight\", 1)\n        )\n        output_tensor = output_tensor.cpu().squeeze(0)\n        output_image = transforms.ToPILImage()(output_tensor)\n        base_name, _ = os.path.splitext(img_file)\n        output_path = os.path.join(config[\"output_dir\"], f\"{base_name}_styled.png\")\n        output_image.save(output_path)\n        print(f\"Saved styled image to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T22:28:43.910951Z","iopub.execute_input":"2025-02-26T22:28:43.911343Z","iopub.status.idle":"2025-02-26T22:28:43.931760Z","shell.execute_reply.started":"2025-02-26T22:28:43.911315Z","shell.execute_reply":"2025-02-26T22:28:43.930754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config.py\n\nconfig = {\n    \"input_dir\": \"/kaggle/input/isolate\",    # Directory containing PNG content images\n    \"output_dir\": \"/kaggle/working/output\",    # Directory to store stylized images\n    \"style_dir\": \"/kaggle/input/earth-inspo\",      # Directory containing style images (only first PNG is used)\n    \"num_steps\": 1000,         # Number of iterations (intensity of style transfer)\n    \"style_weight\": 1_000_000, # Weight for style loss\n    \"content_weight\": 1,       # Weight for content loss\n    \"target_size\": 512,        # (Optional) Resize images to this size\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T22:28:50.721483Z","iopub.execute_input":"2025-02-26T22:28:50.721810Z","iopub.status.idle":"2025-02-26T22:28:50.726011Z","shell.execute_reply.started":"2025-02-26T22:28:50.721787Z","shell.execute_reply":"2025-02-26T22:28:50.724935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run_style_transfer.py\n\nprocess_style_transfer(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T22:31:39.336634Z","iopub.execute_input":"2025-02-26T22:31:39.336941Z","iopub.status.idle":"2025-02-26T22:33:02.148901Z","shell.execute_reply.started":"2025-02-26T22:31:39.336918Z","shell.execute_reply":"2025-02-26T22:33:02.147880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display, Image\n\noutput_image = '/kaggle/working/output/isolate_styled.png'\n\ndisplay(Image(filename=output_image))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T22:33:16.478300Z","iopub.execute_input":"2025-02-26T22:33:16.478703Z","iopub.status.idle":"2025-02-26T22:33:16.501349Z","shell.execute_reply.started":"2025-02-26T22:33:16.478670Z","shell.execute_reply":"2025-02-26T22:33:16.500371Z"}},"outputs":[],"execution_count":null}]}